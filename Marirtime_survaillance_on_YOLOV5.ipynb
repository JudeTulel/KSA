{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 4352009,
          "sourceType": "datasetVersion",
          "datasetId": 756977
        }
      ],
      "dockerImageVersionId": 30746,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JudeTulel/KSA/blob/main/Marirtime_survaillance_on_YOLOV5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combatting illegal maritime fishing on the Kenyan Coast\n",
        "Following the call for applications by the Kenya Space agency to develop Space-Bourne solutions to address the challenges facing the Kenya, we heeded the call and developed a solution to combat illegal maritime fishing on the Kenyan Coast. The solution is a YOLO v5 model trained on a comprehensive dataset of maritime sattelite images comprising of a large-scale fine-grainted dataset of 3,435 images from various sensors, satellite platforms, locations, and seasons."
      ],
      "metadata": {
        "id": "7mGmQbAO5pQb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Dataset\n",
        "For any AI or ML model data plays a paramount role. When speaking of data we talk about its quantity and quality. We chose to make use of the ShipRSImageNet datasetwhich was first presented by Z. Zhang, L. Zhang, Y. Wang, P. Feng and R. He in the paper \"ShipRSImageNet: A Large-Scale Fine-Grained Dataset for Ship Detection in High-Resolution Optical Remote Sensing Images,\" in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, vol. 14, pp. 8458-8472, 2021, doi: 10.1109/JSTARS.2021.3104230.\n",
        "\n",
        "Each image is around 930√ó930 pixels and contains ships with different scales, orientations, and aspect ratios. The images are annotated by experts in satellite image interpretation, categorized into 50 object categories images. The fully annotated ShipRSImageNet contains 17,573 ship instances."
      ],
      "metadata": {
        "id": "uISAxMYzln44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "Clone GitHub [repository](https://github.com/ultralytics/yolov5), install [dependencies](https://github.com/ultralytics/yolov5/blob/master/requirements.txt) and check PyTorch and GPU."
      ],
      "metadata": {
        "id": "ti8HjzxEln44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf sample_data"
      ],
      "metadata": {
        "id": "GibymheNY7zQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5.git  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install"
      ],
      "metadata": {
        "id": "qeBTQ-eCLKsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d2065ef-1548-484d-e859-3759937f795d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 17075, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 17075 (delta 19), reused 7 (delta 7), pack-reused 17049 (from 2)\u001b[K\n",
            "Receiving objects: 100% (17075/17075), 15.68 MiB | 17.57 MiB/s, done.\n",
            "Resolving deltas: 100% (11719/11719), done.\n",
            "/content/yolov5\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m902.2/902.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "execution": {
          "iopub.status.busy": "2024-08-16T05:05:28.238712Z",
          "iopub.execute_input": "2024-08-16T05:05:28.239126Z",
          "iopub.status.idle": "2024-08-16T05:06:15.510833Z",
          "shell.execute_reply.started": "2024-08-16T05:05:28.239093Z",
          "shell.execute_reply": "2024-08-16T05:06:15.508915Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "819e3ff3-bd78-4151-d8f3-923eadd3236e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 üöÄ v7.0-353-g5eca7b9c Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 33.6/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading the dataset"
      ],
      "metadata": {
        "id": "67rKnR61n0AQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download the zip file from google drive\n",
        "\n",
        "!gdown --id '1wApkaSoa9mXRfXQiq6lTtlVrv4cSc6vv' # replace 'your_google_drive_file_id' with actual file id\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjUeJrDzp8S7",
        "outputId": "dc186634-e3b0-487d-f500-fbd5369ce94f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1wApkaSoa9mXRfXQiq6lTtlVrv4cSc6vv\n",
            "From (redirected): https://drive.google.com/uc?id=1wApkaSoa9mXRfXQiq6lTtlVrv4cSc6vv&confirm=t&uuid=27884755-ab6b-4449-aaf2-af22fe9c48d2\n",
            "To: /content/yolov5/ShipRSImageNet_V1.zip\n",
            "100% 4.58G/4.58G [00:45<00:00, 101MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZmfqZyQ_tHfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzipping the dataset\n",
        "!unzip -q ShipRSImageNet_V1.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-SFBmIWvX6F",
        "outputId": "5a4f7599-823a-40b5-9656-ff03900f7b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace ShipRSImageNet_V1/COCO_Format/ShipRSImageNet_bbox_train_level_0.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!cd ShipRSImageNet_V1/\n",
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc91KIzo3E-D",
        "outputId": "022e3a08-69aa-4ab5-e559-fdf59025ae3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov5\n",
            "/content/yolov5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "# Define source and destination directories\n",
        "source_dir = '/content/yolov5/ShipRSImageNet_V1/VOC_Format/JPEGImages/'\n",
        "destination_dir = '/content/yolov5/ShipRSImageNet_V1/COCO_Format/images/'\n",
        "Path(destination_dir).mkdir(parents=True, exist_ok=True)\n",
        "# Iterate over files in source directory\n",
        "for filename in os.listdir(source_dir):\n",
        "  # Construct full file paths\n",
        "  source_path = os.path.join(source_dir, filename)\n",
        "  destination_path = os.path.join(destination_dir, filename)\n",
        "  # Move file\n",
        "  shutil.move(source_path, destination_path)\n"
      ],
      "metadata": {
        "id": "QcJcr_bWy86s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coco json dataset have class id starting from index 1 not zero that need to be fixed"
      ],
      "metadata": {
        "id": "O1vWnSpqX5Kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Define the class names dictionary\n",
        "class_names = {\n",
        "    0: \"AOE\", 1: \"Arleigh Burke DD\", 2: \"Asagiri DD\", 3: \"Atago DD\", 4: \"Austin LL\",\n",
        "    5: \"Barge\", 6: \"Cargo\", 7: \"Commander\", 8: \"Container Ship\", 9: \"DOCK\",\n",
        "    10: \"Enterprise\", 11: \"EPF\", 12: \"Ferry\", 13: \"Fishing Vessel\", 14: \"Hatsuyuki DD\",\n",
        "    15: \"Hovercraft\", 16: \"Hyuga DD\", 17: \"LHA LL\", 18: \"LSD 41 LL\", 19: \"Masyuu AS\",\n",
        "    20: \"Medical ship\", 21: \"Midway\", 22: \"Motorboat\", 23: \"Nimitz\", 24: \"Oil Tanker\",\n",
        "    25: \"Osumi LL\", 26: \"Other Aircraft Carrier\", 27: \"Other Auxiliary Ship\",\n",
        "    28: \"Other Destroyer\", 29: \"Other Frigate\", 30: \"Other Landing\", 31: \"Other Merchant\",\n",
        "    32: \"Other Ship\", 33: \"Other Warship\", 34: \"Patrol\", 35: \"Perry FF\",\n",
        "    36: \"RORO\", 37: \"Sailboat\", 38: \"Sanantonio AS\", 39: \"Submarine\", 40: \"Test ship\",\n",
        "    41: \"Ticonderoga\", 42: \"Training ship\", 43: \"Tugboat\", 44: \"Wasp LL\", 45: \"Yacht\",\n",
        "    46: \"YuDao LL\", 47: \"YuDeng LL\", 48: \"YuTing LL\", 49: \"YuZhao LL\"\n",
        "}\n",
        "\n",
        "def adjust_coco_categories(json_path):\n",
        "    with open(json_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Adjust category ids in 'categories'\n",
        "    for category in data['categories']:\n",
        "        old_id = category['id']\n",
        "        new_id = old_id - 1\n",
        "        category['id'] = new_id\n",
        "        category['name'] = class_names[new_id]\n",
        "\n",
        "    # Adjust category ids in 'annotations'\n",
        "    for ann in data['annotations']:\n",
        "        ann['category_id'] -= 1\n",
        "\n",
        "    # Save the adjusted JSON file\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "# Adjust category IDs in the dataset JSON files\n",
        "json_files = [\n",
        "    '/content/yolov5/ShipRSImageNet_V1/COCO_Format/ShipRSImageNet_bbox_train_level_3.json',\n",
        "    '/content/yolov5/ShipRSImageNet_V1/COCO_Format/ShipRSImageNet_bbox_val_level_3.json',\n",
        "]\n",
        "\n",
        "for json_file in json_files:\n",
        "    adjust_coco_categories(json_file)"
      ],
      "metadata": {
        "id": "hKr9irecX4eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting COCO JSON to YOLOtxt format\n"
      ],
      "metadata": {
        "id": "UAUU5_NaJiSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "def coco_to_yolo(coco_json, images_dir, labels_dir):\n",
        "    # Load COCO JSON\n",
        "    with open(coco_json, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    Path(labels_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Mapping of category ID to category name\n",
        "    category_map = {cat['id']: cat['name'] for cat in coco_data['categories']}\n",
        "\n",
        "    # Process each image\n",
        "    for image in coco_data['images']:\n",
        "        image_id = image['id']\n",
        "        image_filename = image['file_name']\n",
        "        image_path = os.path.join(images_dir, image_filename)\n",
        "        txt_filename = os.path.splitext(image_filename)[0] + '.txt'\n",
        "        txt_path = os.path.join(labels_dir, txt_filename)\n",
        "\n",
        "        # Open the corresponding .txt file for writing\n",
        "        with open(txt_path, 'w') as txt_file:\n",
        "            # Get annotations for this image\n",
        "            annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] == image_id]\n",
        "\n",
        "            for ann in annotations:\n",
        "                # Convert COCO bbox format [x, y, width, height] to YOLO format [x_center, y_center, width, height]\n",
        "                bbox = ann['bbox']\n",
        "                x, y, width, height = bbox\n",
        "                x_center = (x + width / 2) / image['width']\n",
        "                y_center = (y + height / 2) / image['height']\n",
        "                width /= image['width']\n",
        "                height /= image['height']\n",
        "\n",
        "                # Get the class ID and map it to a class name\n",
        "                class_id = ann['category_id']\n",
        "                class_name = category_map[class_id]\n",
        "\n",
        "                # Write to the .txt file in YOLO format\n",
        "                txt_file.write(f\"{class_id} {x_center} {y_center} {width} {height}\\n\")\n",
        "\n",
        "# Parameters\n",
        "coco_json = '/content/yolov5/ShipRSImageNet_V1/COCO_Format/ShipRSImageNet_bbox_train_level_3.json'  # Path to your COCO JSON file\n",
        "images_dir = '/content/yolov5/ShipRSImageNet_V1/COCO_Format/images'  # Path to your images directory\n",
        "labels_dir = '/content/yolov5/ShipRSImageNet_V1/COCO_Format/labels/train'  # Path to the directory where YOLO .txt files should be saved\n",
        "\n",
        "coco_to_yolo(coco_json, images_dir, labels_dir)\n"
      ],
      "metadata": {
        "id": "PTqz13ZNBVqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#for validation data\n",
        "\n",
        "val_json =   '/content/yolov5/ShipRSImageNet_V1/COCO_Format/ShipRSImageNet_bbox_val_level_3.json'\n",
        "images_dir = '/content/yolov5/ShipRSImageNet_V1/COCO_Format/images'\n",
        "labels_dir = '/content/yolov5/ShipRSImageNet_V1/COCO_Format/labels/val'\n",
        "coco_to_yolo(val_json, images_dir, labels_dir)\n"
      ],
      "metadata": {
        "id": "cly8lLltAYkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define source and destination directories\n",
        "source_dir =     '/content/yolov5/ShipRSImageNet_V1/COCO_Format/images'  # Directory containing all images\n",
        "val_labels_dir = '/content/yolov5/ShipRSImageNet_V1/COCO_Format/labels/val'  # Directory with validation labels\n",
        "destination_dir ='/content/yolov5/ShipRSImageNet_V1/COCO_Format/images/val'  # Directory to move validation images\n",
        "\n",
        "# Create destination directory if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Get a list of validation image filenames from the labels\n",
        "val_image_filenames = []\n",
        "for filename in os.listdir(val_labels_dir):\n",
        "    if filename.endswith('.txt'):\n",
        "        val_image_filenames.append(filename.replace('.txt', '.bmp'))\n",
        "\n",
        "# Move validation images\n",
        "for filename in val_image_filenames:\n",
        "    source_path = os.path.join(source_dir, filename)\n",
        "    destination_path = os.path.join(destination_dir, filename)\n",
        "    if os.path.exists(source_path):  # Check if the image exists\n",
        "        shutil.move(source_path, destination_path)\n"
      ],
      "metadata": {
        "id": "QDk-tf_wPZbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#moving the image for train to train folder\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define source and destination directories\n",
        "source_dir =      '/content/yolov5/ShipRSImageNet_V1/COCO_Format/images'  # Directory containing all images\n",
        "val_labels_dir =  '/content/yolov5/ShipRSImageNet_V1/COCO_Format/labels/train'  # Directory with validation labels\n",
        "destination_dir = '/content/yolov5/ShipRSImageNet_V1/COCO_Format/images/train'  # Directory to move validation images\n",
        "# Create destination directory if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Get a list of validation image filenames from the labels\n",
        "val_image_filenames = []\n",
        "for filename in os.listdir(val_labels_dir):\n",
        "    if filename.endswith('.txt'):\n",
        "        val_image_filenames.append(filename.replace('.txt', '.bmp'))\n",
        "\n",
        "# Move validation images\n",
        "for filename in val_image_filenames:\n",
        "    source_path = os.path.join(source_dir, filename)\n",
        "    destination_path = os.path.join(destination_dir, filename)\n",
        "    if os.path.exists(source_path):  # Check if the image exists\n",
        "        shutil.move(source_path, destination_path)\n"
      ],
      "metadata": {
        "id": "S4SCVGS3Sa2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: read the file names in test.txt and move image files with matching names to /content/yolov5/yolov5/ShipRSImageNet_V1/COCO_Format/labels/test\n",
        "\n",
        "# Read filenames from test.txt\n",
        "with open('/content/yolov5/ShipRSImageNet_V1/COCO_Format/test.txt', 'r') as f:\n",
        "    test_filenames = [line.strip() for line in f]\n",
        "\n",
        "# Define source and destination directories\n",
        "source_dir = '/content/yolov5/ShipRSImageNet_V1/COCO_Format/images'\n",
        "destination_dir = '/content/yolov5/ShipRSImageNet_V1/COCO_Format/images/test'\n",
        "\n",
        "# Create destination directory if it doesn't exist\n",
        "os.makedirs(destination_dir, exist_ok=True)\n",
        "\n",
        "# Move test images\n",
        "for filename in test_filenames:\n",
        "    source_path = os.path.join(source_dir, filename + '.bmp')  # Assuming images are .bmp\n",
        "    destination_path = os.path.join(destination_dir, filename + '.bmp')\n",
        "    if os.path.exists(source_path):\n",
        "        shutil.move(source_path, destination_path)\n"
      ],
      "metadata": {
        "id": "DdwbKpZETo0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model\n",
        "Now the the dataset is completly downloaded an extracted into the yolov5 folder we will procced with training\n",
        "\n",
        "We chose to train it"
      ],
      "metadata": {
        "id": "W7AhKkvA1UGM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "klEOA9Z81qbV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train YOLOv5s on COCO128 for 3 epochs\n",
        "!python train.py --img 960 --batch 2 --epochs 14 --data VOC1.yaml --weights yolov5s.pt --cache"
      ],
      "metadata": {
        "id": "pXYNFQ_j1veX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d55841a-8116-4298-84bb-eea2a983c507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-08-19 07:43:15.455714: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-08-19 07:43:15.639642: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-08-19 07:43:15.694048: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=VOC1.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=14, batch_size=2, imgsz=960, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ‚úÖ\n",
            "YOLOv5 üöÄ v7.0-353-g5eca7b9c Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Overriding model.yaml nc=80 with nc=50\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
            "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
            " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
            " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
            " 24      [17, 20, 23]  1    148335  models.yolo.Detect                      [50, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
            "Model summary: 214 layers, 7154479 parameters, 7154479 gradients, 16.4 GFLOPs\n",
            "\n",
            "Transferred 343/349 items from yolov5s.pt\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
            "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.14 (you have 1.4.13). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/yolov5/ShipRSImageNet_V1/COCO_Format/labels/train.cache... 2198 images, 0 backgrounds, 0 corrupt: 100% 2198/2198 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/yolov5/ShipRSImageNet_V1/COCO_Format/images/train/003313.bmp: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/yolov5/ShipRSImageNet_V1/COCO_Format/images/train/1418__0_0.bmp: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/yolov5/ShipRSImageNet_V1/COCO_Format/images/train/2487__0_1498.bmp: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/yolov5/ShipRSImageNet_V1/COCO_Format/images/train/2562__2760_2345.bmp: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/yolov5/ShipRSImageNet_V1/COCO_Format/images/train/2562__3207_1840.bmp: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/yolov5/ShipRSImageNet_V1/COCO_Format/images/train/2562__3207_2345.bmp: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING ‚ö†Ô∏è /content/yolov5/ShipRSImageNet_V1/COCO_Format/images/train/2564__1840_2345.bmp: 2 duplicate labels removed\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (5.2GB ram): 100% 2198/2198 [00:47<00:00, 46.46it/s]\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolov5/ShipRSImageNet_V1/COCO_Format/labels/val.cache... 550 images, 0 backgrounds, 0 corrupt: 100% 550/550 [00:00<?, ?it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /content/yolov5/ShipRSImageNet_V1/COCO_Format/images/val/004153.bmp: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING ‚ö†Ô∏è /content/yolov5/ShipRSImageNet_V1/COCO_Format/images/val/1907__2012_1840.bmp: 1 duplicate labels removed\n",
            "\u001b[34m\u001b[1mval: \u001b[0mCaching images (1.3GB ram): 100% 550/550 [00:13<00:00, 41.89it/s]\n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.10 anchors/target, 0.995 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
            "Plotting labels to runs/train/exp2/labels.jpg... \n",
            "Image sizes 960 train, 960 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
            "Starting training for 14 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       0/13       1.2G    0.08272     0.0637    0.08845         14        960: 100% 1099/1099 [02:41<00:00,  6.82it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:18<00:00,  7.62it/s]\n",
            "                   all        550       3101    0.00596       0.45     0.0166    0.00763\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       1/13       1.2G    0.05554    0.05215    0.07573          5        960: 100% 1099/1099 [02:34<00:00,  7.13it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:17<00:00,  7.76it/s]\n",
            "                   all        550       3101      0.666     0.0797     0.0279     0.0118\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       2/13       1.2G    0.05086    0.04531    0.07237         16        960: 100% 1099/1099 [02:32<00:00,  7.21it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:17<00:00,  7.71it/s]\n",
            "                   all        550       3101      0.657     0.0819     0.0429     0.0209\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       3/13       1.2G    0.04649    0.04575    0.06965         20        960: 100% 1099/1099 [02:31<00:00,  7.27it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:17<00:00,  7.78it/s]\n",
            "                   all        550       3101      0.686      0.101      0.048     0.0238\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       4/13       1.2G    0.04235    0.04312    0.06865          9        960: 100% 1099/1099 [02:30<00:00,  7.31it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:17<00:00,  7.69it/s]\n",
            "                   all        550       3101      0.612      0.127      0.067     0.0368\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       5/13       1.2G    0.04043    0.04351    0.06695         19        960: 100% 1099/1099 [02:30<00:00,  7.32it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:17<00:00,  7.87it/s]\n",
            "                   all        550       3101       0.65      0.128      0.076      0.045\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       6/13       1.2G    0.03772     0.0419    0.06473         14        960: 100% 1099/1099 [02:36<00:00,  7.01it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:18<00:00,  7.49it/s]\n",
            "                   all        550       3101      0.737      0.127      0.084     0.0517\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       7/13       1.2G    0.03688    0.04121    0.06356         69        960: 100% 1099/1099 [02:31<00:00,  7.25it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:17<00:00,  8.07it/s]\n",
            "                   all        550       3101      0.704      0.148       0.11     0.0667\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       8/13       1.2G    0.03611    0.03976    0.06212          8        960: 100% 1099/1099 [02:31<00:00,  7.27it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:17<00:00,  8.04it/s]\n",
            "                   all        550       3101      0.682      0.158      0.116     0.0703\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "       9/13       1.2G    0.03357    0.03815    0.06006          4        960: 100% 1099/1099 [02:29<00:00,  7.34it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:16<00:00,  8.27it/s]\n",
            "                   all        550       3101      0.595      0.201      0.123     0.0772\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      10/13       1.2G    0.03318    0.03789    0.05877         27        960: 100% 1099/1099 [02:30<00:00,  7.30it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:17<00:00,  8.02it/s]\n",
            "                   all        550       3101        0.7      0.178      0.129     0.0802\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      11/13       1.2G    0.03179    0.03766    0.05812         10        960: 100% 1099/1099 [02:30<00:00,  7.29it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:17<00:00,  7.84it/s]\n",
            "                   all        550       3101      0.675      0.182      0.141     0.0912\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      12/13       1.2G    0.03084     0.0368     0.0565          3        960: 100% 1099/1099 [02:31<00:00,  7.26it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:17<00:00,  7.85it/s]\n",
            "                   all        550       3101      0.536      0.261      0.153     0.0997\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
            "      13/13       1.2G    0.03033    0.03604    0.05555         20        960: 100% 1099/1099 [02:29<00:00,  7.37it/s]\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:17<00:00,  7.98it/s]\n",
            "                   all        550       3101      0.521      0.263      0.154      0.104\n",
            "\n",
            "14 epochs completed in 0.704 hours.\n",
            "Optimizer stripped from runs/train/exp2/weights/last.pt, 15.0MB\n",
            "Optimizer stripped from runs/train/exp2/weights/best.pt, 15.0MB\n",
            "\n",
            "Validating runs/train/exp2/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7144975 parameters, 0 gradients, 16.2 GFLOPs\n",
            "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100% 138/138 [00:18<00:00,  7.51it/s]\n",
            "                   all        550       3101      0.521      0.264      0.154      0.104\n",
            "                   AOE        550        297      0.151      0.519      0.189      0.137\n",
            "      Arleigh Burke DD        550        208      0.189      0.649      0.271      0.185\n",
            "            Asagiri DD        550        171      0.538       0.93      0.886      0.513\n",
            "              Atago DD        550          4     0.0469      0.152     0.0457     0.0271\n",
            "             Austin LL        550         18      0.182      0.778      0.514      0.358\n",
            "                 Barge        550         19      0.165      0.737       0.36      0.252\n",
            "                 Cargo        550          5          1          0     0.0181     0.0136\n",
            "             Commander        550         77      0.147      0.818       0.25      0.204\n",
            "        Container Ship        550         46     0.0674     0.0435     0.0709     0.0515\n",
            "                  DOCK        550         41      0.147      0.146      0.114     0.0739\n",
            "            Enterprise        550        130      0.231      0.938      0.392      0.315\n",
            "                   EPF        550         22          1          0     0.0905     0.0639\n",
            "                 Ferry        550         16      0.174      0.438      0.277      0.154\n",
            "        Fishing Vessel        550         14          1          0     0.0603     0.0275\n",
            "          Hatsuyuki DD        550         28          0          0     0.0373     0.0283\n",
            "            Hovercraft        550         77      0.146      0.883      0.241      0.188\n",
            "              Hyuga DD        550         37          1          0      0.439      0.352\n",
            "                LHA LL        550         18          1          0     0.0105    0.00776\n",
            "             LSD 41 LL        550         19      0.469     0.0955       0.24     0.0967\n",
            "             Masyuu AS        550         11          1          0      0.102     0.0718\n",
            "          Medical ship        550          5          1          0     0.0268     0.0213\n",
            "                Midway        550          6          1          0    0.00187    0.00187\n",
            "             Motorboat        550         27          1          0     0.0362     0.0284\n",
            "                Nimitz        550          6          1          0     0.0208     0.0197\n",
            "            Oil Tanker        550          6          1          0     0.0207     0.0174\n",
            "              Osumi LL        550         21          1          0     0.0273     0.0197\n",
            "Other Aircraft Carrier        550         31      0.213      0.581      0.195      0.143\n",
            "  Other Auxiliary Ship        550         32          1          0     0.0479     0.0386\n",
            "       Other Destroyer        550         18          1          0     0.0443     0.0292\n",
            "         Other Frigate        550          5          1          0     0.0127     0.0101\n",
            "         Other Landing        550          7          1          0     0.0116    0.00973\n",
            "        Other Merchant        550         11          1          0     0.0298     0.0264\n",
            "            Other Ship        550         11          1          0     0.0174     0.0128\n",
            "         Other Warship        550          8          1          0    0.00519    0.00377\n",
            "                Patrol        550         13          1          0     0.0169     0.0149\n",
            "              Perry FF        550         10          1          0     0.0137     0.0111\n",
            "                  RORO        550         50     0.0777       0.06     0.0577     0.0348\n",
            "              Sailboat        550         71       0.17      0.732      0.295      0.205\n",
            "         Sanantonio AS        550         20     0.0793        0.5     0.0924     0.0764\n",
            "             Submarine        550        169      0.223      0.893      0.492      0.392\n",
            "             Test ship        550         22          0          0     0.0127    0.00975\n",
            "           Ticonderoga        550         46       0.12      0.478      0.118     0.0786\n",
            "         Training ship        550         53          0          0      0.042     0.0304\n",
            "               Tugboat        550        140       0.32      0.579      0.286       0.18\n",
            "               Wasp LL        550        341      0.552    0.00293     0.0329     0.0192\n",
            "                 Yacht        550         99      0.128     0.0909     0.0514     0.0224\n",
            "              YuDao LL        550         32      0.075      0.188      0.099     0.0729\n",
            "             YuDeng LL        550         31     0.0824      0.839      0.413      0.303\n",
            "             YuTing LL        550        398      0.191       0.49      0.254     0.0942\n",
            "             YuZhao LL        550        154      0.182      0.617       0.34      0.146\n",
            "Results saved to \u001b[1mruns/train/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detect\n",
        "\n",
        "`detect.py` runs YOLOv5 inference on a variety of sources and saving results to `runs/detect`. Example inference sources are:\n",
        "\n",
        "```shell\n",
        "python detect.py --source 0  # webcam\n",
        "                          img.jpg  # image\n",
        "                          vid.mp4  # video\n",
        "                          screen  # screenshot\n",
        "                          path/  # directory\n",
        "                         'path/*.jpg'  # glob\n",
        "                         'https://youtu.be/Zgi9g1ksQHc'  # YouTube\n",
        "                         'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
        "```"
      ],
      "metadata": {
        "id": "4JnkELT0cIJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  !python detect.py --weights /content/yolov5/runs/train/exp/weights/best.pt --img 960 --conf 0.4 --source /content/yolov5/ShipRSImageNet_V1/COCO_Format/images/000019.bmp\n",
        "# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"
      ],
      "metadata": {
        "id": "zR9ZbuQCH7FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a28359-1e6e-4d6c-8625-da2bb6bef294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/yolov5/runs/train/exp/weights/best.pt'], source=/content/yolov5/ShipRSImageNet_V1/COCO_Format/images/000019.bmp, data=data/coco128.yaml, imgsz=[960, 960], conf_thres=0.4, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
            "YOLOv5 üöÄ v7.0-353-g5eca7b9c Python-3.10.12 torch-2.3.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7144975 parameters, 0 gradients, 16.2 GFLOPs\n",
            "WARNING ‚ö†Ô∏è NMS time limit 0.550s exceeded\n",
            "image 1/1 /content/yolov5/ShipRSImageNet_V1/COCO_Format/images/000019.bmp: 960x960 4 AOEs, 4 Arleigh Burke DDs, 1 Enterprise, 22.3ms\n",
            "Speed: 1.0ms pre-process, 22.3ms inference, 583.0ms NMS per image at shape (1, 3, 960, 960)\n",
            "Results saved to \u001b[1mruns/detect/exp2\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Logging\n",
        "\n",
        "Training results are automatically logged with [Tensorboard](https://www.tensorflow.org/tensorboard) and [CSV](https://github.com/ultralytics/yolov5/pull/4148) loggers to `runs/train`, with a new experiment directory created for each new training as `runs/train/exp2`, `runs/train/exp3`, etc.\n",
        "\n",
        "This directory contains train and val statistics, mosaics, labels, predictions and augmentated mosaics, as well as metrics and charts including precision-recall (PR) curves and confusion matrices.\n",
        "\n",
        "<img alt=\"Local logging results\" src=\"https://user-images.githubusercontent.com/26833433/183222430-e1abd1b7-782c-4cde-b04d-ad52926bf818.jpg\" width=\"1280\"/>\n"
      ],
      "metadata": {
        "id": "-WPvRbS5Swl6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exporting ONNX\n",
        "\n"
      ],
      "metadata": {
        "id": "IEijrePND_2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python export.py --weights /content/best.pt --include onnx --simplify"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy0SJF4wqVRq",
        "outputId": "6d336977-0bd5-4adb-b180-6dc3a9c3f435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mexport: \u001b[0mdata=data/coco128.yaml, weights=['/content/best.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, simplify=True, mlmodel=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
            "YOLOv5 üöÄ v7.0-378-g2f74455a Python-3.10.12 torch-2.5.0+cu121 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7144975 parameters, 0 gradients, 16.2 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from /content/best.pt with output shape (1, 25200, 55) (14.3 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.17.0...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.35...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success ‚úÖ 1.5s, saved as /content/best.onnx (27.7 MB)\n",
            "\n",
            "Export complete (2.5s)\n",
            "Results saved to \u001b[1m/content\u001b[0m\n",
            "Detect:          python detect.py --weights /content/best.onnx \n",
            "Validate:        python val.py --weights /content/best.onnx \n",
            "PyTorch Hub:     model = torch.hub.load('ultralytics/yolov5', 'custom', '/content/best.onnx')  \n",
            "Visualize:       https://netron.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/best.onnx.zip /content/best.onnx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcTrgSkdqtzu",
        "outputId": "6bf23cb4-e84b-4e27-d4f9-945b3092e9af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/best.onnx (deflated 16%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "SqMSulCSLBru",
        "outputId": "58c1a0c0-911b-4bfe-f4ce-788b03972138",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (4.25.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def preprocess_image(image_path, input_size=(640, 640)):\n",
        "    \"\"\"Load and preprocess an image for ONNX inference.\"\"\"\n",
        "    image = cv2.imread(image_path)\n",
        "    image_resized = cv2.resize(image, input_size)\n",
        "    image_rgb = cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB)\n",
        "    image_normalized = image_rgb.astype(np.float32) / 255.0\n",
        "    image_transposed = np.transpose(image_normalized, (2, 0, 1))\n",
        "    image_batched = np.expand_dims(image_transposed, axis=0)\n",
        "    return image_batched, image  # Preprocessed and original image\n",
        "\n",
        "def postprocess_output(output, input_size=(640, 640), conf_threshold=0.5, iou_threshold=0.45):\n",
        "    \"\"\"Decode YOLO model output and filter detections.\"\"\"\n",
        "    predictions = output[0]  # (1, 25200, 55)\n",
        "    predictions = predictions[0]  # Remove the batch dimension -> (25200, 55)\n",
        "\n",
        "    # Extract bounding box coordinates, object confidence, and class scores\n",
        "    box_coords = predictions[:, :4]  # x_center, y_center, width, height\n",
        "    object_conf = predictions[:, 4:5]  # Objectness confidence\n",
        "    class_scores = predictions[:, 5:]  # Class probabilities\n",
        "\n",
        "    # Compute final scores: object confidence * class scores\n",
        "    scores = object_conf * class_scores  # Shape: (25200, 50)\n",
        "\n",
        "    # Filter out low-confidence detections\n",
        "    detections = []\n",
        "    for i in range(predictions.shape[0]):\n",
        "        class_id = np.argmax(scores[i])  # Get class with the highest score\n",
        "        confidence = scores[i, class_id]\n",
        "        if confidence > conf_threshold:\n",
        "            x_center, y_center, width, height = box_coords[i]\n",
        "            x1 = int((x_center - width / 2) * input_size[0])\n",
        "            y1 = int((y_center - height / 2) * input_size[1])\n",
        "            x2 = int((x_center + width / 2) * input_size[0])\n",
        "            y2 = int((y_center + height / 2) * input_size[1])\n",
        "            detections.append([x1, y1, x2, y2, confidence, class_id])\n",
        "\n",
        "    # Apply Non-Maximum Suppression\n",
        "    return nms(detections, iou_threshold)\n",
        "\n",
        "def nms(detections, iou_threshold=0.45):\n",
        "    \"\"\"Apply Non-Maximum Suppression to reduce overlapping detections.\"\"\"\n",
        "    detections = sorted(detections, key=lambda x: x[4], reverse=True)  # Sort by confidence\n",
        "    final_detections = []\n",
        "\n",
        "    while detections:\n",
        "        best_box = detections.pop(0)\n",
        "        final_detections.append(best_box)\n",
        "        detections = [\n",
        "            box for box in detections\n",
        "            if iou(best_box, box) < iou_threshold\n",
        "        ]\n",
        "    return final_detections\n",
        "\n",
        "def iou(box1, box2):\n",
        "    \"\"\"Calculate Intersection over Union (IoU) between two boxes.\"\"\"\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    return inter_area / (box1_area + box2_area - inter_area)\n",
        "\n",
        "\n",
        "def nms(detections, iou_threshold=0.45):\n",
        "    \"\"\"Apply Non-Maximum Suppression.\"\"\"\n",
        "    detections = sorted(detections, key=lambda x: x[4], reverse=True)  # Sort by confidence\n",
        "    final_detections = []\n",
        "\n",
        "    while detections:\n",
        "        best_box = detections.pop(0)\n",
        "        final_detections.append(best_box)\n",
        "        detections = [\n",
        "            box for box in detections\n",
        "            if iou(best_box, box) < iou_threshold\n",
        "        ]\n",
        "    return final_detections\n",
        "\n",
        "def iou(box1, box2):\n",
        "    \"\"\"Calculate Intersection over Union (IoU) between two boxes.\"\"\"\n",
        "    x1 = max(box1[0], box2[0])\n",
        "    y1 = max(box1[1], box2[1])\n",
        "    x2 = min(box1[2], box2[2])\n",
        "    y2 = min(box1[3], box2[3])\n",
        "\n",
        "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
        "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    return inter_area / (box1_area + box2_area - inter_area)\n",
        "\n",
        "def draw_bounding_boxes(image, detections, input_size):\n",
        "    \"\"\"Draw bounding boxes on the image.\"\"\"\n",
        "    for x1, y1, x2, y2, confidence, class_id in detections:\n",
        "        label = f\"Class {int(class_id)}: {confidence:.2f}\"\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        cv2.putText(image, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "    return image\n",
        "\n",
        "def run_inference(model_path, image_path):\n",
        "    \"\"\"Run inference on a single image.\"\"\"\n",
        "    session = ort.InferenceSession(model_path)\n",
        "    input_name = session.get_inputs()[0].name\n",
        "    input_shape = session.get_inputs()[0].shape[2:]  # HxW\n",
        "\n",
        "    input_data, original_image = preprocess_image(image_path, input_size=tuple(input_shape))\n",
        "\n",
        "    outputs = session.run(None, {input_name: input_data})\n",
        "\n",
        "    results = postprocess_output(outputs, input_size=input_shape)\n",
        "    return draw_bounding_boxes(original_image, results, input_shape)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_file = \"/content/ShipDetectionClassifier.onnx\"\n",
        "    test_image = \"/content/test.bmp\"\n",
        "    output_image_path = \"/content/output_with_boxes.bmp\"\n",
        "\n",
        "    image_with_boxes = run_inference(model_file, test_image)\n",
        "    cv2.imwrite(output_image_path, image_with_boxes)\n",
        "    print(f\"Saved output image with bounding boxes to {output_image_path}\")\n"
      ],
      "metadata": {
        "id": "Sb-ujvExGO6_",
        "outputId": "c3cc39ef-29b1-410f-f9a3-559e829b12e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output: [array([[[5.03198147e+00, 3.44854450e+00, 1.44361839e+01, ...,\n",
            "         1.30154788e-02, 3.39886010e-01, 3.36927176e-03],\n",
            "        [1.20452061e+01, 4.84836578e+00, 2.71010437e+01, ...,\n",
            "         2.30700672e-02, 2.23187178e-01, 5.02443314e-03],\n",
            "        [1.88362179e+01, 5.72505283e+00, 3.39703636e+01, ...,\n",
            "         3.01379263e-02, 1.84780478e-01, 7.74091482e-03],\n",
            "        ...,\n",
            "        [5.74004517e+02, 6.10347900e+02, 1.40105820e+02, ...,\n",
            "         1.00424290e-02, 8.49562883e-03, 3.00862134e-01],\n",
            "        [5.96260376e+02, 6.13508057e+02, 1.15028030e+02, ...,\n",
            "         1.41535997e-02, 1.21980906e-02, 1.21688396e-01],\n",
            "        [6.17779907e+02, 6.19278076e+02, 1.42883530e+02, ...,\n",
            "         1.46721303e-02, 1.24725401e-02, 6.71646297e-02]]], dtype=float32)]\n",
            "Detections Array Shape: (1, 25200, 55)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.10.0) /io/opencv/modules/imgcodecs/src/loadsave.cpp:798: error: (-215:Assertion failed) !_img.empty() in function 'imwrite'\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d5c7ade2ffec>\u001b[0m in \u001b[0;36m<cell line: 61>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Save and display the output image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_image_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_with_boxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saved output image with bounding boxes to {output_image_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.10.0) /io/opencv/modules/imgcodecs/src/loadsave.cpp:798: error: (-215:Assertion failed) !_img.empty() in function 'imwrite'\n"
          ]
        }
      ]
    }
  ]
}